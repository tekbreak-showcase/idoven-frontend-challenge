# How to start
#### Requirements
  1. Docker and Docker compose
#### Let's go  
 1. Clone the repo
 2. Run `docker-compose up -d --build`
 3. Grab a ☕ and visit http://localhost:3000


# Architecture
The web application is served using NextJS (Typescript + React). MongoDB is used to store the ECG data. Highcharts is the third-party library implemented for displaying the ECG data.

Using Docker, this system automates the decompression and parsing of the ECG data stored in the folder ‘data’ as compressed (7z) ECG data files.

I decided to take the dockerization approach firstly to provide the app with a quick database. Later, I saw advantages in letting you know how I manipulated the data from the very beginning (raw data from the 7z file) because every data manipulation step matters.

The Docker compose configuration creates 2 servers:
1.  MongoDB. The one holding the database with the parsed ECG data
2.  Client. The one serving the application to a browser


# The Data
The data goes through several steps which are described here. The first data state is compressed and the last state is displayed in a chart in a browser.

Initially every 7z file in the /data folder is decompressed and the output is stored as a txt file containing the data with a CSV format. Then the parser `parser.py` takes the CSV and strips out all the “unnecessary” data (please check the following point Data strip). In the process, the parser generates 3 files:

1.  Seed file. Containing mongo-friendly data to be imported smoothly.
2.  Initial data file. This file contains an overview of the full data. (We will come back to this point later in the Speed and Accuracy section)
3.  Length file. A simple file containing the amount of samples which time is an exact second.

Having all these files created, both servers can start. As soon as the Client server is up and running, the first post-start script is the seeder ‘seed.py’. The seeder will look into the folder ‘ecg-data’ for files generated by the parser. For each data id, the seeder will create a collection to import all the data inside it.

Once all this is finished, after 2-3 minutes, the App will be ready in [http://localhost:3000](http://localhost:3000)

## Decisions:
### Data strip
The unit of the time values stored in the database is centisecond because the sample step is 40 millisecond. someThis way some data will be saved not only during the manipulation and DB seeding phase, but in the following 2 scenarios:
  1.  For every single API data transfer when the database is queried.
  2.  When the query result is transmitted to the client.

 This change does not affect the final result at all

### Speed and accuracy
Other kinds of data strip does affect the result, the `accuracy`, but is it worth it? Big amounts of data is saved in transfers (hence improving the performance) by setting the environment variable `accuracy`.

This ENV variable affects the API and sets the maximum window size to provide granular samples. Requested time ranges bigger than this value will be served as a discrete response of seconds only.

The `accuracy` variable can be then set to a proper value taking into consideration the data and both server and clients performance. In this project, the value is set to 2 hours (7200000 milliseconds) which offers the maximum of 4MB for every request to the API to get points in a range of 2 hours.

### Pre-start scripts:
The pre scripts add execution time for the deployments, but it is a one-time-thing you can benefit a lot from.

In the pre-start scripts (client.Dockerfile) the data is extracted and parsed. Also some files are generated on the go (Note 1) to improve the performance on both the client and server side. The pre-start scripts prepare the system and the data for the Client application to start working.


## Goals

Since the beginning, my mind started to make plans about how I can achieve the challenge. Lots of ideas came up and these are some of the achieved ones:
* From 130 MB of unreadable compressed data (1.5 GB before compression) to a fully queriable MongoDB collection of only 50MB.
* Acceptable Highchart look&feel
* Complete external chart control system
* Put it all together in a “bottle”

## Wished
* Time for unit tests and e2e tests
* A better and more complete control of the chart from an external component
* Panning. Being able to drag and drop the chart to change the extremes.
* A better Highcharts React integration

## Notes
1.  While parsing the raw data, I could create the companion files with just 4 seconds more of execution

## References
* Highcharts https://highcharts.com
* NextJS https://github.com/vercel/next.js/tree/canary/examples/with-mongodb
